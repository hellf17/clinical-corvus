###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off
import baml_py
from enum import Enum
from pydantic import BaseModel, ConfigDict
from typing_extensions import TypeAlias
from typing import Dict, Generic, List, Literal, Optional, TypeVar, Union


T = TypeVar('T')
CheckName = TypeVar('CheckName', bound=str)

class Check(BaseModel):
    name: str
    expression: str
    status: str

class Checked(BaseModel, Generic[T,CheckName]):
    value: T
    checks: Dict[CheckName, Check]

def get_checks(checks: Dict[CheckName, Check]) -> List[Check]:
    return list(checks.values())

def all_succeeded(checks: Dict[CheckName, Check]) -> bool:
    return all(check.status == "succeeded" for check in get_checks(checks))



class ResearchSourceType(str, Enum):
    
    PUBMED = "PUBMED"
    EUROPE_PMC = "EUROPE_PMC"
    LENS_SCHOLARLY = "LENS_SCHOLARLY"
    WEB_SEARCH_BRAVE = "WEB_SEARCH_BRAVE"
    COCHRANE = "COCHRANE"
    CLINICAL_TRIALS_GOV = "CLINICAL_TRIALS_GOV"
    ACADEMIC_GOOGLE_SCHOLAR = "ACADEMIC_GOOGLE_SCHOLAR"
    ACADEMIC_NCBI = "ACADEMIC_NCBI"
    ACADEMIC_ELITE_JOURNAL = "ACADEMIC_ELITE_JOURNAL"
    ACADEMIC_DATABASE_GENERAL = "ACADEMIC_DATABASE_GENERAL"
    GUIDELINE_RESOURCE = "GUIDELINE_RESOURCE"
    PREPRINT = "PREPRINT"

class StudyTypeFilter(str, Enum):
    
    ALL = "ALL"
    SYSTEMATIC_REVIEW = "SYSTEMATIC_REVIEW"
    RANDOMIZED_CONTROLLED_TRIAL = "RANDOMIZED_CONTROLLED_TRIAL"
    COHORT_STUDY = "COHORT_STUDY"
    CASE_CONTROL = "CASE_CONTROL"
    CLINICAL_TRIAL = "CLINICAL_TRIAL"
    REVIEW = "REVIEW"

class UserRole(str, Enum):
    
    PATIENT = "PATIENT"
    DOCTOR_STUDENT = "DOCTOR_STUDENT"

class AnalyzeDifferentialDiagnosesSNAPPSInput(BaseModel):
    case_summary: str
    student_differential_diagnoses: List[str]
    case_context: str

class AnswerProbeQuestionsSNAPPSInputModel(BaseModel):
    session_context: str
    student_questions: str
    case_data: str

class AnsweredQuestion(BaseModel):
    question: str
    answer: str
    rationale: str

class BAMLProblemRepresentationInput(BaseModel):
    full_patient_narrative: str
    user_problem_representation: str
    user_semantic_qualifiers: List[str]

class BenchmarkComparison(BaseModel):
    industry_percentile: float
    performance_grade: str
    benchmark_insights: List[str]
    improvement_targets: List[str]

class BiasAssessment(BaseModel):
    bias_type: str
    risk_level: str
    explanation: str
    mitigation_strategies: List[str]

class CaseContext(BaseModel):
    demographics: str
    chief_complaint: str
    physical_exam: str
    vital_signs: str
    full_description: str
    expected_differentials: Optional[List[str]] = None
    learning_objectives: Optional[List[str]] = None
    expert_analysis: Optional[str] = None

class CaseScenarioInput(BaseModel):
    case_vignette: str
    initial_findings: List["ClinicalFinding"]
    plausible_hypotheses: List[str]

class CiteSourceAnalysisInput(BaseModel):
    search_results: List["RawSearchResultItem"]
    query: str
    include_visualization_data: Optional[bool] = None

class CiteSourceAnalysisOutput(BaseModel):
    deduplication_summary: "DeduplicationSummary"
    source_performance: List["SourcePerformanceMetrics"]
    quality_assessment: "QualityScores"
    processing_insights: List[str]
    recommendations: List[str]
    deduplicated_results: List["RawSearchResultItem"]
    processing_metadata: "ProcessingMetadata"

class CiteSourceMetrics(BaseModel):
    total_sources_consulted: int
    original_results_count: int
    deduplicated_results_count: int
    deduplication_rate: float
    overall_quality_score: float
    coverage_score: float
    diversity_score: float
    recency_score: float
    impact_score: float
    source_balance_score: float
    best_performing_source: str
    processing_time_ms: float
    key_quality_insights: List[str]

class CiteSourceReportInput(BaseModel):
    query: str
    max_results: Optional[int] = None
    include_detailed_metrics: Optional[bool] = None
    include_recommendations: Optional[bool] = None

class CiteSourceReportOutput(BaseModel):
    executive_summary: "ExecutiveSummary"
    quality_breakdown: "QualityScores"
    source_analysis: "SourceAnalysis"
    deduplication_analysis: "DeduplicationAnalysis"
    actionable_insights: List[str]
    benchmark_comparison: "BenchmarkComparison"
    visual_data_summary: Optional["VisualDataSummary"] = None
    disclaimer: str

class ClinicalApplicationAssessment(BaseModel):
    clinical_relevance: str
    patient_population_match: str
    outcome_relevance: str
    practical_feasibility: str
    cost_effectiveness_considerations: Optional[str] = None
    ethical_considerations: List[str]

class ClinicalDataInput(BaseModel):
    patient_story: str
    known_findings: List["ClinicalFinding"]
    patient_demographics: str

class ClinicalFinding(BaseModel):
    finding_name: str
    details: Optional[str] = None
    onset_duration_pattern: Optional[str] = None
    severity_level: Optional[str] = None

class ClinicalScenarioInput(BaseModel):
    clinical_scenario: str
    additional_context: Optional[str] = None

class ClinicalWorkflowQuestionsOutput(BaseModel):
    question_categories: List["QuestionCategory"]
    red_flag_questions: List[str]
    overall_rationale: str

class CognitiveBiasInput(BaseModel):
    case_summary_by_user: str
    user_working_hypothesis: str
    user_reasoning_summary: str

class CognitiveBiasReflectionOutput(BaseModel):
    potential_biases_to_consider: List["DetectedCognitiveBias"]

class CompareContrastExerciseInput(BaseModel):
    scenario: "CaseScenarioInput"
    student_analysis: List["StudentHypothesisAnalysis"]

class CompareContrastFeedbackOutput(BaseModel):
    overall_feedback: Optional[str] = None
    detailed_feedback_per_hypothesis: List["HypothesisComparisonFeedback"]
    suggested_learning_focus: Optional[str] = None

class DdxEvaluation(BaseModel):
    diagnosis: str
    plausibility: str
    supporting_findings: List[str]
    contradicting_findings: List[str]

class DdxQuestioningInput(BaseModel):
    chief_complaint: str
    initial_findings: List["ClinicalFinding"]
    patient_demographics: str

class DdxQuestioningOutput(BaseModel):
    prioritized_questions: List[str]
    complementary_questions: List[str]
    questioning_rationale: str
    potential_systems_to_explore: List[str]

class DeduplicationAnalysis(BaseModel):
    efficiency_metrics: "DeduplicationSummary"
    duplication_patterns: List[str]
    source_overlap_analysis: str
    optimization_suggestions: List[str]

class DeduplicationSummary(BaseModel):
    original_count: int
    deduplicated_count: int
    removed_duplicates: int
    deduplication_rate: float
    efficiency_score: float

class DetectedCognitiveBias(BaseModel):
    bias_type: str
    explanation_as_question: str
    mitigation_prompt: str

class DiagnosticTimeoutInput(BaseModel):
    case_description: str
    current_working_diagnosis: str
    time_elapsed_minutes: Optional[int] = None
    complexity_level: Optional[str] = None

class DiagnosticTimeoutOutput(BaseModel):
    timeout_recommendation: str
    alternative_diagnoses_to_consider: List[str]
    key_questions_to_ask: List[str]
    red_flags_to_check: List[str]
    next_steps_suggested: List[str]
    cognitive_checks: List[str]

class DifferentialAnalysisOutputModel(BaseModel):
    ddx_evaluation: List["DdxEvaluation"]
    missing_differentials: List[str]
    prioritization_feedback: str
    socratic_questions: List[str]
    next_step_guidance: str

class EnhancedAppraisalOutput(BaseModel):
    identified_study_type: str
    study_design_appropriateness: str
    quality_assessment: "QualityAssessment"
    methodological_strengths: List[str]
    methodological_limitations: List[str]
    bias_assessments: List["BiasAssessment"]
    overall_bias_risk: str
    statistical_assessment: "StatisticalAssessment"
    clinical_application: "ClinicalApplicationAssessment"
    pico_alignment_assessment: str
    population_match_percentage: str
    intervention_comparability: str
    outcome_relevance_score: str
    strength_of_recommendation: str
    level_of_evidence: str
    confidence_in_findings: str
    recommendations_for_practice: List[str]
    areas_requiring_more_research: List[str]
    next_steps_for_evidence_evaluation: List[str]
    key_clinical_considerations: List[str]
    potential_harms_or_risks: List[str]
    patient_preference_factors: List[str]
    generalizability_assessment: str
    external_validity_concerns: List[str]
    study_limitations_impact: str
    learning_points: List[str]
    critical_appraisal_checklist: List[str]
    evidence_synthesis_date: str

class EvaluateManagementPlanSNAPPSInputModel(BaseModel):
    session_context: str
    student_plan: str
    case_data: str

class EvaluateSummarySNAPPSInputModel(BaseModel):
    student_summary: str
    case_description: str

class EvidenceAppraisalInput(BaseModel):
    clinical_question_PICO: str
    evidence_summary_or_abstract: str
    study_type_if_known: Optional[str] = None

class EvidenceTheme(BaseModel):
    theme_name: str
    key_findings: List[str]
    strength_of_evidence: str
    supporting_studies_count: int

class ExecutiveSummary(BaseModel):
    total_sources_consulted: int
    deduplication_efficiency_rate: float
    overall_quality_grade: str
    best_performing_source: str
    key_strengths: List[str]
    key_improvement_areas: List[str]

class ExpandDifferentialDiagnosisInput(BaseModel):
    presenting_complaint: str
    location_if_pain: Optional[str] = None
    student_initial_ddx_list: List[str]

class ExpandedDdxOutput(BaseModel):
    applied_approach_description: str
    suggested_additional_diagnoses_with_rationale: List[str]

class FacilitateDDxAnalysisOutputModel(BaseModel):
    response: str

class FacilitateDDxAnalysisSNAPPSInputModel(BaseModel):
    case_summary: str
    differential_diagnoses: List[str]
    student_analysis: str
    case_context: str

class FormulatedSearchStrategyOutput(BaseModel):
    refined_query_for_llm_synthesis: str
    search_parameters_list: List["SearchParameters"]
    search_rationale: str
    expected_evidence_types: List[str]

class HypothesisComparisonFeedback(BaseModel):
    hypothesis_name: str
    feedback_on_supporting_findings: Optional[str] = None
    feedback_on_refuting_findings: Optional[str] = None
    feedback_on_discriminators: Optional[str] = None
    expert_comparison_points: Optional[List[str]] = None

class IllnessScriptInput(BaseModel):
    disease_name: str

class IllnessScriptOutput(BaseModel):
    disease_name: str
    predisposing_conditions: List[str]
    pathophysiology_summary: str
    key_symptoms_and_signs: List[str]
    relevant_diagnostics: Optional[List[str]] = None

class LabAnalysisInput(BaseModel):
    lab_results: List["LabTestResult"]
    user_role: "UserRole"
    patient_context: Optional[str] = None
    specific_user_query: Optional[str] = None

class LabInsightsOutput(BaseModel):
    patient_friendly_summary: Optional[str] = None
    potential_health_implications_patient: Optional[List[str]] = None
    lifestyle_tips_patient: Optional[List[str]] = None
    questions_to_ask_doctor_patient: Optional[List[str]] = None
    important_results_to_discuss_with_doctor: Optional[List[str]] = None
    professional_detailed_reasoning_cot: Optional[str] = None
    key_abnormalities_professional: Optional[List[str]] = None
    key_normal_results_with_context: Optional[List[str]] = None
    potential_patterns_and_correlations: Optional[List[str]] = None
    differential_considerations_professional: Optional[List[str]] = None
    suggested_next_steps_professional: Optional[List[str]] = None

class LabTestResult(BaseModel):
    test_name: str
    value: str
    unit: Optional[str] = None
    reference_range_low: Optional[str] = None
    reference_range_high: Optional[str] = None
    interpretation_flag: Optional[str] = None
    notes: Optional[str] = None

class PDFAnalysisInput(BaseModel):
    pdf_content: str
    analysis_focus: Optional[str] = None
    clinical_question: Optional[str] = None

class PDFAnalysisOutput(BaseModel):
    document_type: str
    key_findings: List[str]
    methodology_summary: str
    clinical_relevance: str
    evidence_quality: str
    recommendations: List[str]
    limitations: List[str]
    structured_summary: str

class PICOFormulationOutput(BaseModel):
    structured_pico_question: "PICOQuestion"
    explanation: str
    pico_derivation_reasoning: str
    search_terms_suggestions: List[str]
    boolean_search_strategies: List[str]
    alternative_pico_formulations: Optional[List[str]] = None
    recommended_study_types: List[str]

class PICOQuestion(BaseModel):
    patient_population: str
    intervention: str
    comparison: Optional[str] = None
    outcome: str
    time_frame: Optional[str] = None
    study_type: Optional[str] = None

class PatientFollowUpChecklistOutput(BaseModel):
    checklist_items: List[str]
    when_to_contact_doctor_urgently: Optional[List[str]] = None
    general_advice: Optional[str] = None

class PatientFollowUpInput(BaseModel):
    consultation_summary_or_concept_explained: str
    doctor_recommendations_summary: Optional[str] = None

class PlanEvaluationOutputModel(BaseModel):
    plan_strengths: List[str]
    plan_gaps: List[str]
    investigation_priorities: List[str]
    management_considerations: List[str]
    safety_concerns: List[str]
    cost_effectiveness_notes: List[str]
    guidelines_alignment: str
    next_step_guidance: str

class ProbeResponseOutputModel(BaseModel):
    answers_to_questions: List["AnsweredQuestion"]
    additional_considerations: List[str]
    counter_questions: List[str]
    knowledge_gaps_identified: List[str]
    learning_resources: List[str]

class ProblemRepresentationFeedbackOutputModel(BaseModel):
    feedback_strengths: List[str]
    feedback_improvements: List[str]
    missing_elements: List[str]
    overall_assessment: str
    next_step_guidance: str
    socratic_questions: List[str]

class ProcessingMetadata(BaseModel):
    total_processing_time_ms: float
    sources_analyzed: int
    timestamp: str
    version: str

class ProvideSessionSummarySNAPPSInputModel(BaseModel):
    session_history: List[str]
    case_context: str
    student_selected_topic: Optional[str] = None

class QualityAssessment(BaseModel):
    overall_quality_grade: str
    methodological_rigor_score: str
    risk_of_bias_summary: str
    applicability_concerns: List[str]
    reporting_quality: str

class QualityScores(BaseModel):
    overall_score: float
    coverage_score: float
    diversity_score: float
    recency_score: float
    impact_score: float
    source_balance_score: float

class QuestionCategory(BaseModel):
    category_name: str
    questions: List[str]
    category_rationale: str

class RawSearchResultItem(BaseModel):
    source: "ResearchSourceType"
    title: str
    url: Optional[str] = None
    snippet_or_abstract: str
    publication_date: Optional[str] = None
    authors: Optional[List[str]] = None
    journal: Optional[str] = None
    pmid: Optional[str] = None
    doi: Optional[str] = None
    study_type: Optional[str] = None
    citation_count: Optional[int] = None
    relevance_score: Optional[float] = None
    composite_impact_score: Optional[float] = None
    academic_source_name: Optional[str] = None

class ResearchMetrics(BaseModel):
    total_articles_analyzed: int
    sources_consulted: List[str]
    search_duration_seconds: float
    quality_filters_applied: List[str]
    date_range_searched: Optional[str] = None
    language_filters_applied: Optional[List[str]] = None
    search_strategy_summary: Optional[str] = None
    unique_journals_found: int
    high_impact_studies_count: int
    recent_studies_count: int
    systematic_reviews_count: int
    rct_count: int
    cite_source_metrics: Optional["CiteSourceMetrics"] = None

class ResearchTaskInput(BaseModel):
    user_original_query: str
    pico_question: Optional["PICOQuestion"] = None
    research_focus: Optional[str] = None
    target_audience: Optional[str] = None
    research_mode: Optional[str] = None

class SearchParameters(BaseModel):
    source: "ResearchSourceType"
    query_string: str
    max_results: Optional[int] = None
    study_type_filter: Optional["StudyTypeFilter"] = None
    date_range_years: Optional[int] = None
    language_filter: Optional[str] = None
    rationale: Optional[str] = None

class SessionSummaryOutputModel(BaseModel):
    overall_performance: str
    key_strengths: List[str]
    areas_for_development: List[str]
    learning_objectives_met: List[str]
    recommended_study_topics: List[str]
    metacognitive_insights: List[str]
    next_cases_suggestions: List[str]

class SimplifiedQueryOutput(BaseModel):
    simplified_query: str

class SourceAnalysis(BaseModel):
    source_rankings: List["SourcePerformanceMetrics"]
    coverage_analysis: str
    diversity_assessment: str
    performance_insights: List[str]

class SourcePerformanceMetrics(BaseModel):
    source_name: str
    total_results: int
    unique_contributions: int
    quality_score: float
    response_time_ms: float
    recent_publications_count: int
    high_impact_count: int

class StatisticalAssessment(BaseModel):
    sample_size_adequacy: str
    statistical_methods_appropriateness: str
    effect_size_interpretation: str
    confidence_intervals_assessment: str
    p_value_interpretation: str
    multiple_comparisons_concern: Optional[str] = None

class StructuredSummaryOutput(BaseModel):
    one_sentence_summary: str
    semantic_qualifiers_identified: List[str]
    key_patient_details_abstracted: List[str]
    suggested_areas_for_further_data_gathering: List[str]

class StudentHypothesisAnalysis(BaseModel):
    hypothesis_name: str
    supporting_findings: List[str]
    refuting_findings: List[str]
    key_discriminators_against_others: List[str]

class SummaryFeedbackOutputModel(BaseModel):
    feedback_strengths: List[str]
    feedback_improvements: List[str]
    missing_elements: List[str]
    overall_assessment: str
    next_step_guidance: str
    socratic_questions: List[str]

class SynthesizedResearchOutput(BaseModel):
    original_query: str
    executive_summary: str
    professional_detailed_reasoning_cot: str
    clinical_implications: List[str]
    key_findings_by_theme: List["EvidenceTheme"]
    research_gaps_identified: List[str]
    evidence_quality_assessment: str
    relevant_references: List["RawSearchResultItem"]
    research_metrics: Optional["ResearchMetrics"] = None
    search_duration_seconds: Optional[float] = None

class TranslationOutput(BaseModel):
    translated_text: str

class VisualDataSummary(BaseModel):
    chart_data_available: bool
    supported_visualizations: List[str]
    data_summary: str
